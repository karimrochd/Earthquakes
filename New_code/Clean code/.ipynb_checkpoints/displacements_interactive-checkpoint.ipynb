{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:17:58.395577Z",
     "start_time": "2024-07-05T09:17:58.391185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utilities' from '/mnt/beegfs/projects/corrQuake/Earthquakes/New_code/Clean code/utilities.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "import h5py\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "import utilities\n",
    "\n",
    "reload(utilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74758964b85c2d7a",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1d1bd582791723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:17:58.422459Z",
     "start_time": "2024-07-05T09:17:58.405627Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_spatio_temporally_close_mainshocks(mainshocks_days_dict, mainshocks_locations_dict, grid_size_km,\n",
    "                                              min_days_between_mainshocks):\n",
    "    \"\"\" Filter main shocks that are close in space and time, keeping only the one that happened first. \"\"\"\n",
    "    earth_radius_km = 6371  # km\n",
    "    filtered_keys = set(mainshocks_days_dict.keys())  # Start with all keys\n",
    "    to_remove = set()\n",
    "\n",
    "    keys = sorted(mainshocks_days_dict.keys(), key=lambda x: mainshocks_days_dict[x])  # Sort keys by date\n",
    "\n",
    "    # Compare each pair of main shocks\n",
    "    n_keys = len(keys)\n",
    "    print(f\"Number of main shocks: {n_keys}\")\n",
    "    for i in range(n_keys - 1):\n",
    "        for j in range(i + 1, n_keys):\n",
    "            key_i, key_j = keys[i], keys[j]\n",
    "            if key_i in to_remove or key_j in to_remove:\n",
    "                continue  # Skip if already marked for removal\n",
    "\n",
    "            day_i, day_j = mainshocks_days_dict[key_i], mainshocks_days_dict[key_j]\n",
    "            loc_i, loc_j = mainshocks_locations_dict[key_i], mainshocks_locations_dict[key_j]\n",
    "\n",
    "            # Calculate spatial distance\n",
    "            distance = haversine_distances(np.radians(loc_i).reshape(-1, 2),\n",
    "                                           np.radians(loc_j).reshape(-1, 2)) * earth_radius_km\n",
    "\n",
    "            # Check if they are close in space and time\n",
    "            if (distance <= np.sqrt(2) * grid_size_km / 2) and ((day_j - day_i) < np.timedelta64(\n",
    "                    min_days_between_mainshocks, 'D')):\n",
    "                to_remove.add(key_j)  # Remove the later one\n",
    "\n",
    "    # Update filtered keys by removing those marked\n",
    "    filtered_keys.difference_update(to_remove)\n",
    "    print(f\"Number of main shocks after filtering spatiotemporally close mainshocks: {len(filtered_keys)}\")\n",
    "    return filtered_keys\n",
    "\n",
    "\n",
    "def filter_gps_stations(earthquakes, ngl_list, search_radius, min_mainshock_mag, min_stations_per_main_shock, region):\n",
    "    \"\"\" \n",
    "    Filter GPS stations based on their distance to the main shocks \n",
    "    and the number of stations around each main shock.\n",
    "    Morally, 'earthquakes' is the catalog, with labeled sequences (in 'seq_id')\n",
    "    ngl_list: metadata of the GPS stations\n",
    "    \"\"\"\n",
    "    stations_to_download = set()\n",
    "    mainshocks_stations = {}\n",
    "    mainshocks_days = {}\n",
    "    main_shocks_locations = {}\n",
    "    region_min_lat, region_max_lat, region_min_lon, region_max_lon = utilities.return_regions()[region]\n",
    "    earth_radius_km = 6371  # km\n",
    "\n",
    "    for id_seq, seq in earthquakes.groupby('seq_id'):\n",
    "        if len(seq) <= 1:\n",
    "            continue\n",
    "        mainshock = seq[(seq['type'] == 1) & (seq['mag'] >= min_mainshock_mag) & (seq['lat'] >= region_min_lat) & (\n",
    "                seq['lat'] <= region_max_lat) & (seq['lon'] >= region_min_lon) & (seq['lon'] <= region_max_lon)]\n",
    "        if mainshock.empty:\n",
    "            continue\n",
    "        distances = haversine_distances(np.radians(ngl_list[[\"lat\", \"lon\"]]),\n",
    "                                        np.radians(mainshock[[\"lat\", \"lon\"]].values))[:, 0]\n",
    "        distances *= earth_radius_km\n",
    "        ## distances : list of all distances station-mainshock\n",
    "        mainshock_day = mainshock.day.values[0]\n",
    "        mainshock_location = mainshock[['lat', 'lon']].values[0]\n",
    "        main_shocks_locations[id_seq] = mainshock_location\n",
    "        # We only consider stations that were active n_days_before_mainshock days before the main shock \n",
    "        # and n_days_after_mainshock days after and that are within the max_radius distance from the main shock\n",
    "        valid_mask = (distances <= search_radius) & (\n",
    "                ngl_list.begin.values <= (mainshock_day - np.timedelta64(n_days_before_mainshock, 'D'))) & (\n",
    "                             ngl_list.end.values >= (mainshock_day + np.timedelta64(n_days_after_mainshock,\n",
    "                                                                                    'D')))\n",
    "\n",
    "        valid_stations = ngl_list.name[valid_mask]\n",
    "        if len(valid_stations) >= min_stations_per_main_shock:\n",
    "            stations_to_download.update(valid_stations)\n",
    "            mainshocks_stations[id_seq] = list(valid_stations)\n",
    "            mainshocks_days[id_seq] = mainshock_day\n",
    "\n",
    "    ## filter mainshocks that are too close in space and time\n",
    "    filtered_keys = filter_spatio_temporally_close_mainshocks(mainshocks_days, main_shocks_locations, grid_size_km,\n",
    "                                                              min_days_between_mainshocks)\n",
    "    mainshocks_stations = {k: mainshocks_stations[k] for k in filtered_keys}\n",
    "    mainshocks_days = {k: mainshocks_days[k] for k in filtered_keys}\n",
    "    return stations_to_download, mainshocks_stations, mainshocks_days\n",
    "\n",
    "\n",
    "def download_and_filter_gps_data(stations_to_download, gps_data_output_path, mainshocks_stations, mainshocks_days,\n",
    "                                 n_days_before_mainshock, n_days_after_mainshock, ngl_list,\n",
    "                                 mainshocks_gps_stations_daily_positions_path=\"\"):\n",
    "    \"\"\"\n",
    "    Download and filter GPS data for the stations around the main shocks. \n",
    "    \n",
    "    \"\"\"\n",
    "    if os.path.exists(mainshocks_gps_stations_daily_positions_path):\n",
    "        mainshocks_gps_stations_daily_positions = joblib.load(mainshocks_gps_stations_daily_positions_path)\n",
    "        return mainshocks_gps_stations_daily_positions\n",
    "    mainshocks_gps_stations_daily_positions = {}  ## GPS data at all relevant days, for each given mainshock\n",
    "    if not stations_to_download:\n",
    "        raise ValueError(\"No stations to download.\")\n",
    "\n",
    "    # We calculate the median latitude and longitude of the stations to download to use as a reference point\n",
    "    # of the region\n",
    "    filtered_ngl_list = ngl_list[ngl_list.name.isin(stations_to_download)]\n",
    "    center_lat = filtered_ngl_list['lat'].median()\n",
    "    center_lon = filtered_ngl_list['lon'].median()\n",
    "    ## median of all stations seen in all mainshocks, i.e. approximately center of Japan\n",
    "\n",
    "    # Load or download data for each station once and store it in a dictionary\n",
    "    stations_daily_positions = {}\n",
    "    for station in stations_to_download:\n",
    "        file_path = os.path.join(gps_data_output_path, f\"{station}.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            one_station_daily_positions = pd.read_csv(file_path, sep=\" \", parse_dates=['date'])\n",
    "            print(f\"{station} data loaded.\")\n",
    "        else:\n",
    "            try:\n",
    "                one_station_daily_positions = utilities.get_ngl_gps_data(station, \"tenv3\")\n",
    "                utilities.convert_lat_lon_to_km(one_station_daily_positions, center_lat, center_lon)\n",
    "                ## data[km_lat] and data[km_lon] are now filled\n",
    "                one_station_daily_positions.to_csv(file_path, sep=\" \", index=False)\n",
    "                print(f\"{station} data downloaded and saved.\")\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error downloading data for station {station}: {e}\")\n",
    "\n",
    "        stations_daily_positions[station] = one_station_daily_positions  ## temporary dict\n",
    "\n",
    "    # Pre-calculate the date ranges for each main shock\n",
    "    mainshock_date_ranges = {\n",
    "        id_seq: (\n",
    "            mainshocks_days[id_seq] - np.timedelta64(n_days_before_mainshock, 'D'),\n",
    "            mainshocks_days[id_seq] + np.timedelta64(n_days_after_mainshock, 'D')\n",
    "        )\n",
    "        for id_seq in mainshocks_days.keys()\n",
    "    }\n",
    "\n",
    "    # Process each main shock\n",
    "    for id_seq, stations in mainshocks_stations.items():\n",
    "        start_date, end_date = mainshock_date_ranges[id_seq]\n",
    "        directory_path = f\"{gps_data_output_path}/Sequence_{id_seq}\"\n",
    "        os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "        for station in stations:\n",
    "            if station not in stations_daily_positions:\n",
    "                raise ValueError(f\"Data for station {station} not found.\")\n",
    "\n",
    "            ## subdata: morally, the GPS data restricted to a window, for 1 mainshock\n",
    "            one_station_daily_positions_restricted_window_path = f\"{directory_path}/{station}_positions.csv\"\n",
    "            if os.path.exists(one_station_daily_positions_restricted_window_path):\n",
    "                one_station_daily_positions_restricted_window = pd.read_csv(\n",
    "                    one_station_daily_positions_restricted_window_path, sep=\" \", parse_dates=['date'])\n",
    "                one_station_daily_positions_restricted_window = one_station_daily_positions_restricted_window[\n",
    "                    (one_station_daily_positions_restricted_window['date'] >= start_date) & (\n",
    "                            one_station_daily_positions_restricted_window['date'] <= end_date)]\n",
    "\n",
    "                print(f\"{station} subdata around main shock {id_seq} loaded.\")\n",
    "            else:\n",
    "                one_station_daily_positions = stations_daily_positions[station]\n",
    "                one_station_daily_positions_restricted_window = one_station_daily_positions[\n",
    "                    (one_station_daily_positions['date'] >= start_date) & (\n",
    "                            one_station_daily_positions['date'] <= end_date)]\n",
    "                one_station_daily_positions_restricted_window = one_station_daily_positions_restricted_window.sort_values(\n",
    "                    by='date')\n",
    "\n",
    "                if (\n",
    "                        len(one_station_daily_positions_restricted_window) == n_days_before_mainshock + 1 + n_days_after_mainshock) and np.isfinite(\n",
    "                    one_station_daily_positions_restricted_window[\n",
    "                        ['lat', 'lon', 'z', 'km_lat', 'km_lon']].values).all():\n",
    "                    one_station_daily_positions_restricted_window.to_csv(\n",
    "                        one_station_daily_positions_restricted_window_path, sep=\" \", index=False)\n",
    "                    print(f\"{station} subdata around main shock {id_seq} saved.\")\n",
    "                else:\n",
    "                    print(f\"Data of station {station} around main shock {id_seq} is incomplete or contains NaNs.\")\n",
    "                    continue\n",
    "\n",
    "            if id_seq not in mainshocks_gps_stations_daily_positions:\n",
    "                mainshocks_gps_stations_daily_positions[id_seq] = []\n",
    "            mainshocks_gps_stations_daily_positions[id_seq].append(one_station_daily_positions_restricted_window)\n",
    "\n",
    "    joblib.dump(mainshocks_gps_stations_daily_positions, mainshocks_gps_stations_daily_positions_path)\n",
    "    return mainshocks_gps_stations_daily_positions\n",
    "\n",
    "\n",
    "def process_and_save_main_shocks_data(mainshocks_gps_stations_daily_positions, mainshocks_days, earthquakes,\n",
    "                                      min_after_shock_mag,\n",
    "                                      after_shock_time_window,\n",
    "                                      n_days_after_mainshock,\n",
    "                                      hdf5_output_file_path,\n",
    "                                      min_stations_per_main_shock):\n",
    "    \"\"\" Process and save the main shocks data to the HDF5 file. \"\"\"\n",
    "    earth_radius_km = 6371  # km\n",
    "    km_per_degree_lat = np.pi * earth_radius_km / 180.0\n",
    "    for id_seq, ms_gps_stations_positions in mainshocks_gps_stations_daily_positions.items():\n",
    "        print(f\"Processing main shock {id_seq}...\")\n",
    "        displacements = []\n",
    "        stations_positions = []\n",
    "\n",
    "        # Extract aftershocks information\n",
    "        seq = earthquakes[earthquakes.seq_id == id_seq]  ## all EQs of that sequence\n",
    "        mainshock = seq[seq['type'] == 1 & (seq['day'] == mainshocks_days[id_seq])]\n",
    "        # Filter aftershocks\n",
    "        aftershocks = seq[(seq['type'] == 2) & (\n",
    "                seq['day'] > mainshocks_days[id_seq] + np.timedelta64(n_days_after_mainshock, 'D')) & (\n",
    "                                  seq['day'] <= mainshocks_days[id_seq] + np.timedelta64(after_shock_time_window, 'D'))\n",
    "                          & (seq['mag'] >= min_after_shock_mag)]\n",
    "        if len(aftershocks) < 1: continue\n",
    "        mainshock_location = mainshock[['lat', 'lon']].values[0]\n",
    "        one_ms_aftershocks_locations = aftershocks[['lat', 'lon']].values\n",
    "        one_ms_aftershocks_mags = aftershocks['mag'].values\n",
    "\n",
    "        # loop over all stations of a given main shock, and for each compute the displacements\n",
    "        for gps_station_positions in ms_gps_stations_positions:\n",
    "            station_position = \\\n",
    "                gps_station_positions[\n",
    "                    gps_station_positions['date'] == mainshocks_days[id_seq] - np.timedelta64(1, 'D')][\n",
    "                    ['lat', 'lon']].values[0]  ## position of the station at day -1 \n",
    "            stations_positions.append(station_position)\n",
    "            \n",
    "\n",
    "            dis_lat = (\n",
    "                    gps_station_positions['lat'].shift(-1) - gps_station_positions['lat'].shift(1)\n",
    "            ).dropna().values\n",
    "            dis_lat *= km_per_degree_lat  ## convert to km\n",
    "            origin_lat = gps_station_positions['lat'].median()\n",
    "            km_per_degree_lon = km_per_degree_lat * np.cos(np.radians(origin_lat))\n",
    "            dis_lon = (\n",
    "                    gps_station_positions['lon'].shift(-1) - gps_station_positions['lon'].shift(1)\n",
    "            ).dropna().values\n",
    "            dis_lon *= km_per_degree_lon  ## convert to km\n",
    "            dis_z = (\n",
    "                    gps_station_positions['z'].shift(-1) - gps_station_positions['z'].shift(1)\n",
    "            ).dropna().values\n",
    "\n",
    "            # Stack the displacements into a single numpy array\n",
    "            # Each column corresponds to a displacement dimension (lat, lon, z)\n",
    "            station_displacements = np.stack((dis_lat, dis_lon, dis_z), axis=-1)\n",
    "            if not np.isfinite(station_displacements).all(): continue\n",
    "            displacements.append(station_displacements)\n",
    "        if len(displacements) < min_stations_per_main_shock: continue  \n",
    "        # Save to HDF5\n",
    "        with h5py.File(hdf5_output_file_path, 'a') as f:\n",
    "            if str(id_seq) in f:\n",
    "                del f[str(id_seq)]\n",
    "            grp = f.create_group(str(id_seq))\n",
    "            grp.attrs['main_shock_day'] = str(np.datetime_as_string(mainshocks_days[id_seq], unit='D'))\n",
    "            grp.attrs['main_shock_magnitude'] = mainshock.mag.values[0]\n",
    "            grp.attrs['main_shock_location'] = mainshock_location\n",
    "\n",
    "            grp.create_dataset('gps_stations_displacements', data=np.array(displacements))\n",
    "            grp.create_dataset('stations_positions', data=np.vstack(stations_positions))\n",
    "            grp.create_dataset('aftershocks_magnitudes', data=one_ms_aftershocks_mags)\n",
    "            grp.create_dataset('aftershocks_locations', data=one_ms_aftershocks_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04b2bdca5a44b7e",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d519a20b56f7a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:17:58.426441Z",
     "start_time": "2024-07-05T09:17:58.423805Z"
    }
   },
   "outputs": [],
   "source": [
    "earthquakes_path = \"custom_catalog.csv\"  #Path to the earthquake catalog file\n",
    "search_radius = 300  #Search radius of GPS stations in km\n",
    "min_mainshock_mag = 6  #Minimum magnitude of mainshocks to consider\n",
    "min_stations_per_main_shock = 3  #Minimum number of GPS stations around a mainshock to consider it\n",
    "regression = True  #Whether to use regression or classification\n",
    "if regression:\n",
    "    min_after_shock_mag = 2.5  #Minimum magnitude of aftershocks to consider\n",
    "else:\n",
    "    min_after_shock_mag = 4\n",
    "after_shock_time_window = 45  #Days after the mainshock to search for aftershocks\n",
    "n_days_before_mainshock = 1  #Number of days before the mainshock to consider\n",
    "n_days_after_mainshock = 1  #Number of days after the mainshock to consider\n",
    "min_days_between_mainshocks = 30  #Minimum number of days between two mainshocks to consider them as separate events\n",
    "grid_size_km = 250  # Size of the square grid in km \n",
    "# path to the hdf5 file to store the data for the main shocks\n",
    "hdf5_output_file_path = f\"Data/Displacements_min_mainshock_mag={min_mainshock_mag}_min_stations_per_main_shock={min_stations_per_main_shock}_regression={regression}_min_after_shock_mag={min_after_shock_mag}_after_shock_time_window={after_shock_time_window}_n_days_before_mainshock={n_days_before_mainshock}_n_days_after_mainshock={n_days_after_mainshock}_min_days_between_mainshocks={min_days_between_mainshocks}_grid_size_km={grid_size_km}.hdf5\"\n",
    "# path to the dictionary containing the GPS data for the stations around the main shocks\n",
    "dict_mainshocks_gps_stations_daily_positions_path = \"Data/dict_mainshocks_gps_stations_daily_positions.pkl\"\n",
    "region = 'Japan'  # Region of the main shocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ca56106eee0f0",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce46ca2d7535fc01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:17:58.809745Z",
     "start_time": "2024-07-05T09:17:58.427214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngl stations already downloaded\n"
     ]
    }
   ],
   "source": [
    "# utilities.initialize_hdf5(hdf5_output_file_path) # not necessary, the file is created when the first main shock is processed\n",
    "\n",
    "## load catalog\n",
    "os.makedirs(\"Data\", exist_ok=True)\n",
    "earthquakes = pd.read_csv(earthquakes_path, sep=\" \", parse_dates=['datetime'])\n",
    "earthquakes.rename(columns={'datetime': 'day'}, inplace=True)\n",
    "earthquakes['day'] = earthquakes.day.values.astype('datetime64[D]')\n",
    "earthquakes.sort_values(by='day', inplace=True)\n",
    "earthquakes.reset_index(drop=True, inplace=True)\n",
    "\n",
    "## load list of stations (all available stations, many that are not used)\n",
    "gps_data_output_path = \"Data\"  #Path to store the GPS data\n",
    "ngl_list = utilities.get_ngl_stations(f'{gps_data_output_path}/ngl_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecbfac36844ee298",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:18:01.153292Z",
     "start_time": "2024-07-05T09:17:58.810893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of main shocks: 319\n",
      "Number of main shocks after filtering spatiotemporally close mainshocks: 179\n"
     ]
    }
   ],
   "source": [
    "## detect which stations are relevant\n",
    "stations_to_download, mainshocks_stations, mainshocks_days = filter_gps_stations(earthquakes, ngl_list,\n",
    "                                                                                 search_radius, min_mainshock_mag,\n",
    "                                                                                 min_stations_per_main_shock, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b856fd1fcd4257c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:18:05.399890Z",
     "start_time": "2024-07-05T09:18:01.154163Z"
    }
   },
   "outputs": [],
   "source": [
    "## download stations' data (positions) and filter by day-windows\n",
    "## if csv's are in the proper location, then no download is node and it's very fast\n",
    "\n",
    "# Create the folder\n",
    "mainshocks_gps_stations_daily_positions = download_and_filter_gps_data(stations_to_download, gps_data_output_path,\n",
    "                                                                       mainshocks_stations,\n",
    "                                                                       mainshocks_days, n_days_before_mainshock,\n",
    "                                                                       n_days_after_mainshock,\n",
    "                                                                       ngl_list,\n",
    "                                                                       dict_mainshocks_gps_stations_daily_positions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2975b45e7ebe9c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:18:19.404503Z",
     "start_time": "2024-07-05T09:18:05.400655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing main shock 57...\n",
      "Processing main shock 69...\n",
      "Processing main shock 89...\n",
      "Processing main shock 90...\n",
      "Processing main shock 103...\n",
      "Processing main shock 104...\n",
      "Processing main shock 108...\n",
      "Processing main shock 109...\n",
      "Processing main shock 114...\n",
      "Processing main shock 118...\n",
      "Processing main shock 151...\n",
      "Processing main shock 153...\n",
      "Processing main shock 155...\n",
      "Processing main shock 156...\n",
      "Processing main shock 158...\n",
      "Processing main shock 161...\n",
      "Processing main shock 163...\n",
      "Processing main shock 172...\n",
      "Processing main shock 174...\n",
      "Processing main shock 176...\n",
      "Processing main shock 177...\n",
      "Processing main shock 179...\n",
      "Processing main shock 181...\n",
      "Processing main shock 182...\n",
      "Processing main shock 187...\n",
      "Processing main shock 194...\n",
      "Processing main shock 198...\n",
      "Processing main shock 199...\n",
      "Processing main shock 200...\n",
      "Processing main shock 203...\n",
      "Processing main shock 204...\n",
      "Processing main shock 206...\n",
      "Processing main shock 207...\n",
      "Processing main shock 213...\n",
      "Processing main shock 214...\n",
      "Processing main shock 215...\n",
      "Processing main shock 217...\n",
      "Processing main shock 218...\n",
      "Processing main shock 219...\n",
      "Processing main shock 223...\n",
      "Processing main shock 226...\n",
      "Processing main shock 228...\n",
      "Processing main shock 233...\n",
      "Processing main shock 236...\n",
      "Processing main shock 248...\n",
      "Processing main shock 251...\n",
      "Processing main shock 252...\n",
      "Processing main shock 256...\n",
      "Processing main shock 257...\n",
      "Processing main shock 258...\n",
      "Processing main shock 265...\n",
      "Processing main shock 267...\n",
      "Processing main shock 271...\n",
      "Processing main shock 275...\n",
      "Processing main shock 277...\n",
      "Processing main shock 279...\n",
      "Processing main shock 280...\n",
      "Processing main shock 282...\n",
      "Processing main shock 283...\n",
      "Processing main shock 284...\n",
      "Processing main shock 285...\n",
      "Processing main shock 291...\n",
      "Processing main shock 293...\n",
      "Processing main shock 294...\n",
      "Processing main shock 295...\n",
      "Processing main shock 296...\n",
      "Processing main shock 297...\n",
      "Processing main shock 299...\n",
      "Processing main shock 300...\n",
      "Processing main shock 302...\n",
      "Processing main shock 306...\n",
      "Processing main shock 308...\n",
      "Processing main shock 311...\n",
      "Processing main shock 312...\n",
      "Processing main shock 314...\n",
      "Processing main shock 315...\n",
      "Processing main shock 317...\n",
      "Processing main shock 318...\n",
      "Processing main shock 319...\n",
      "Processing main shock 320...\n",
      "Processing main shock 321...\n",
      "Processing main shock 324...\n",
      "Processing main shock 325...\n",
      "Processing main shock 335...\n",
      "Processing main shock 352...\n",
      "Processing main shock 376...\n",
      "Processing main shock 378...\n",
      "Processing main shock 392...\n",
      "Processing main shock 393...\n",
      "Processing main shock 410...\n",
      "Processing main shock 414...\n",
      "Processing main shock 415...\n",
      "Processing main shock 418...\n",
      "Processing main shock 420...\n",
      "Processing main shock 422...\n",
      "Processing main shock 424...\n",
      "Processing main shock 426...\n",
      "Processing main shock 427...\n",
      "Processing main shock 429...\n",
      "Processing main shock 432...\n",
      "Processing main shock 433...\n",
      "Processing main shock 435...\n",
      "Processing main shock 436...\n",
      "Processing main shock 437...\n",
      "Processing main shock 438...\n",
      "Processing main shock 439...\n",
      "Processing main shock 440...\n",
      "Processing main shock 442...\n",
      "Processing main shock 443...\n",
      "Processing main shock 446...\n",
      "Processing main shock 447...\n",
      "Processing main shock 448...\n",
      "Processing main shock 450...\n",
      "Processing main shock 451...\n",
      "Processing main shock 452...\n",
      "Processing main shock 456...\n",
      "Processing main shock 457...\n",
      "Processing main shock 458...\n",
      "Processing main shock 460...\n",
      "Processing main shock 461...\n",
      "Processing main shock 463...\n",
      "Processing main shock 466...\n",
      "Processing main shock 467...\n",
      "Processing main shock 468...\n",
      "Processing main shock 471...\n",
      "Processing main shock 473...\n",
      "Processing main shock 480...\n",
      "Processing main shock 481...\n",
      "Processing main shock 484...\n",
      "Processing main shock 485...\n",
      "Processing main shock 487...\n",
      "Processing main shock 489...\n",
      "Processing main shock 490...\n",
      "Processing main shock 491...\n",
      "Processing main shock 492...\n",
      "Processing main shock 494...\n",
      "Processing main shock 496...\n",
      "Processing main shock 504...\n",
      "Processing main shock 505...\n",
      "Processing main shock 508...\n",
      "Processing main shock 510...\n",
      "Processing main shock 512...\n",
      "Processing main shock 513...\n",
      "Processing main shock 514...\n",
      "Processing main shock 517...\n",
      "Processing main shock 518...\n",
      "Processing main shock 525...\n",
      "Processing main shock 526...\n",
      "Processing main shock 527...\n",
      "Processing main shock 531...\n",
      "Processing main shock 532...\n",
      "Processing main shock 533...\n",
      "Processing main shock 534...\n",
      "Processing main shock 537...\n",
      "Processing main shock 538...\n",
      "Processing main shock 539...\n",
      "Processing main shock 540...\n",
      "Processing main shock 541...\n",
      "Processing main shock 543...\n",
      "Processing main shock 544...\n",
      "Processing main shock 546...\n",
      "Processing main shock 551...\n",
      "Processing main shock 552...\n",
      "Processing main shock 553...\n",
      "Processing main shock 554...\n",
      "Processing main shock 556...\n",
      "Processing main shock 557...\n",
      "Processing main shock 562...\n",
      "Processing main shock 563...\n",
      "Processing main shock 565...\n",
      "Processing main shock 566...\n",
      "Processing main shock 567...\n",
      "Processing main shock 568...\n",
      "Processing main shock 570...\n",
      "Processing main shock 572...\n",
      "Processing main shock 573...\n",
      "Processing main shock 575...\n",
      "Processing main shock 576...\n",
      "Processing main shock 578...\n"
     ]
    }
   ],
   "source": [
    "process_and_save_main_shocks_data(mainshocks_gps_stations_daily_positions, mainshocks_days, earthquakes,\n",
    "                                  min_after_shock_mag,\n",
    "                                  after_shock_time_window,\n",
    "                                  n_days_after_mainshock,\n",
    "                                  hdf5_output_file_path,\n",
    "                                  min_stations_per_main_shock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3c24f-3c35-4448-a1c0-a2be2d1e028b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
